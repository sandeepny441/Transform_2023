{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "This notebook contains two questions with multiple parts. Please refer to the provided *problem statement* for these questions.\n",
    "\n",
    "A count-up timer is located on the lower lefthand corner of this page. After 48 hours, this assessment will be automatically submitted and made read-only.\n",
    "\n",
    "To submit your notebooks before the 48 hours have elapsed, return to https://modeling.hddatascience.us and click \"Complete Course...\" next to where you launched this server.\n",
    "\n",
    "For support, please contact tech@hddatascience.us\n",
    "    \n",
    "    \n",
    "### Supplemental documents\n",
    "    \n",
    "There are three reference documents that will be used in the questions.\n",
    "    \n",
    "<ol>\n",
    "    <li> <a href=\"Data Dictionary.csv\">Data Dictionary.csv </a> - a data dictionary that describes the fields of the following datasets</li>\n",
    "    <li> <a type=\"text/csv\" href=\"Property Level Data.csv\" target=\"_blank\">Property Level Data.csv </a>  - a dataset containing property level data</li>\n",
    "    <li> <a href=\"Census Level Data.csv\">Census Level Data.csv </a> - a dataset containing census level data</li>\n",
    "    \n",
    "</ol>\n",
    "\n",
    "Review the documents.  The files are found within the root notebook folder and can be loaded from code as needed.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing packages\n",
    "<p>  \n",
    "     You may import the packages of your chosing. Most common package are already installed on the server.  If you are not able to import the package, you may install packages using <code> !{sys.executable} -m pip install <package> </code> for python and <code> install.packages(\"forecaset\")</code> for R within a cell as needed.</p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install shap==0.38.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_jobs Hyperparameter\n",
    "<p>\n",
    "In case model(s) you choose require(s) setting up the <code>n_jobs</code> hyperparameter, please set <code>n_jobs=1</code> or <code>n_jobs=4</code>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "If you are new to Jupyter Notebooks, please see this <a href=\"https://nbviewer.jupyter.org/github/jupyter/notebook/blob/master/docs/source/examples/Notebook/Running%20Code.ipynb\"> documentation </a> for more information on how to run code and use the environment. Alternatively, click on the \"Help\" dropdown menu at the top of this page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may complete this notebook in either python or R. To change the kernal from python to R, go the Kernel Menu and select \"Change Kernel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the the cell below to import your Python packages.  You may add additional packages to import as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add imports as needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using python\n",
    "df_main = pd.read_csv(r\"Property Level Data.csv\") # Load main data\n",
    "df_census = pd.read_csv(r\"Census Level Data.csv\") # Load census data\n",
    "\n",
    "#If using R\n",
    "#df_main = read.csv('Property Level Data.csv') # Load main data\n",
    "#df_census = read.csv('Census Level Data.csv') # Load census data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block pheader\">\n",
    "        <span class=\"heading\">Question 1</span> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: (TP + TN) / (TP + TN + FP + FN) = (5387 + 2011) / (5387 + 2011 + 2105 + 497) = 0.74. This means the model correctly predicted the outcome for 74% of the properties.\n",
    "\n",
    "Precision: TP / (TP + FP) = 5387 / (5387 + 2105) = 0.72. This means that when the model predicted a property would succeed, it was correct 72% of the time.\n",
    "\n",
    "Recall (Sensitivity): TP / (TP + FN) = 5387 / (5387 + 497) = 0.92. This means the model correctly identified 92% of the successful properties.\n",
    "\n",
    "Specificity: TN / (TN + FP) = 2011 / (2011 + 2105) = 0.49. This means the model correctly identified 49% of the unsuccessful properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Net Benefit: -928000000\n"
     ]
    }
   ],
   "source": [
    "# Define the costs and benefits\n",
    "benefit = 1000000\n",
    "cost = 3000000\n",
    "\n",
    "# Calculate the expected net benefit\n",
    "expected_net_benefit = 5387 * benefit - 2105 * cost\n",
    "\n",
    "print(f'Expected Net Benefit: {expected_net_benefit}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative value indicates that the costs associated with investing in unsuccessful properties (as predicted by the model) outweigh the benefits from successful properties.\n",
    "\n",
    "Given that the cost of an unsuccessful property is three times higher than the benefit of a successful property, the model needs to have a high precision (i.e., a low false positive rate) to achieve a positive net benefit. However, based on the confusion matrix, the model has a relatively high number of false positives (2105), which leads to a high cost that outweighs the benefits from true positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    " <span class=\"heading\">Interview Candidate Problem Statement </span>\n",
    "</div>\n",
    "\n",
    "<p>\n",
    "The Real Estate team has approached you to help predict which properties they should invest in. They have compiled market data, containing thousands of properties. It is assumed that if a property is successful, on average it will yield a \\$1,000,000 benefit. Conversely, it is assumed that if a property is unsuccessful, on average it will cost the business \\$3,000,000.  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please insert Markdown cells for discussion and Code cells for your codes as necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block pheader\">\n",
    "        <span class=\"heading\">Question 2</span> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please insert Markdown cells for discussion and Code cells for your codes as necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Question: 2\n",
    "\n",
    "# Examine the data \n",
    "# Handle missing values using imputations\n",
    "# Encode Categorical and normalize numerical variables\n",
    "# Build Classification models\n",
    "# Perform cost benifit analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.head(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census.head(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling missing values\n",
    "\n",
    "df_main['AnnualAverageRent'].fillna(df_main['AnnualAverageRent'].median(), inplace=True)\n",
    "df_main['PropertyValue'].fillna(df_main['PropertyValue'].median(), inplace=True)\n",
    "\n",
    "for column in ['ExpenseTax', 'ExpenseRepairs', 'ExpenseInsurance', 'ExpensePayroll', 'ExpenseGeneralFees']:\n",
    "    df_main[column + '_is_missing'] = df_main[column].isna().astype(int)\n",
    "    df_main[column].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "df_main = pd.get_dummies(df_main, columns=['PropertyType', 'PropertySubType'])\n",
    "\n",
    "# Ordinal encoding for 'ParkingRatio'\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ord_enc = OrdinalEncoder()\n",
    "df_main['ParkingRatio'] = ord_enc.fit_transform(df_main[['ParkingRatio']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling of numerical values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_main[['BuildingCount', 'StoryCount', 'YearBuilt', 'UnitCount', 'NetRentableSF', 'YearLastRenovated', 'GrossLandArea', 'OccupancyPercentage']] = scaler.fit_transform(df_main[['BuildingCount', 'StoryCount', 'YearBuilt', 'UnitCount', 'NetRentableSF', 'YearLastRenovated', 'GrossLandArea', 'OccupancyPercentage']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#featuree engg\n",
    "df_main['AgeOfProperty'] = pd.datetime.now().year - df_main['YearBuilt']\n",
    "df_main['YearsSinceRenovated'] = pd.datetime.now().year - df_main['YearLastRenovated']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge both datasets on StateCode\n",
    "df = pd.merge(df_main, df_census, left_on='StateCode', right_on='STATECODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visulasations for various columns in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['BuildingCount', 'StoryCount', 'YearBuilt', 'UnitCount', 'NetRentableSF', 'YearLastRenovated', 'GrossLandArea', 'OccupancyPercentage']].hist(bins=30, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(data=df[['BuildingCount', 'StoryCount', 'YearBuilt', 'UnitCount', 'NetRentableSF', 'YearLastRenovated', 'GrossLandArea', 'OccupancyPercentage']])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "df['PropertyType_Multifamily'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of PropertyType_Multifamily')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "df['PropertySubType_COOP'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of PropertyType_Multifamily')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "df['PropertySubType_Co-op'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of PropertyType_Multifamily')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "df['PropertySubType_student_housing'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of PropertyType_Multifamily')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 22))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='NetRentableSF', y='UnitCount', data=df)\n",
    "plt.title('NetRentableSF vs. UnitCount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building -- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your features and target variable\n",
    "X = df.drop('SuccesssProb', axis=1)  # assuming 'SuccesssProb' is your target variable\n",
    "y = df['SuccesssProb']\n",
    "\n",
    "# Convert 'SuccesssProb' into a binary variable\n",
    "y = (df['SuccesssProb'] > 0.5).astype(int)\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model\n",
    "model_lr = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model_lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model_lr.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'ROC AUC Score: {roc_auc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Make probability predictions on the test set\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Classify properties as successful or unsuccessful based on the optimal decision threshold\n",
    "y_pred = (y_prob > min_threshold).astype(int)\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm  = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix for better visualization\n",
    "cm_df = pd.DataFrame(cm, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost benifit analysis\n",
    "import numpy as np\n",
    "\n",
    "# Define the costs and benefits\n",
    "benefit = 1000000\n",
    "cost = 3000000\n",
    "\n",
    "# Calculate the probabilities of the positive class\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the expected cost for different decision thresholds\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "expected_costs = []\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_prob > threshold).astype(int)\n",
    "    fp = np.sum((y_test == 0) & (y_pred == 1))\n",
    "    tp = np.sum((y_test == 1) & (y_pred == 1))\n",
    "    expected_cost = fp * cost - tp * benefit\n",
    "    expected_costs.append(expected_cost)\n",
    "\n",
    "# Find the threshold with the minimum expected cost\n",
    "min_cost = min(expected_costs)\n",
    "min_threshold = thresholds[expected_costs.index(min_cost)]\n",
    "\n",
    "print(f'Minimum Expected Cost: {min_cost}')\n",
    "print(f'Optimal Decision Threshold: {min_threshold}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold value of approximately 0.76.  is the optimal decision threshold for classifying a property as successful or unsuccessful, based on minimizing the expected cost.\n",
    "\n",
    "This means that if the model predicts a success probability of greater than 0.76 for a property, \n",
    "we should classify it as successful; otherwise, you should classify it as unsuccessful.\n",
    "\n",
    "Generally, the accuracy is 50%, but based on our conditions of 1:3 ratio of profit to loss,\n",
    "we ended up at 0.76 as threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Initialize the model\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Make probability predictions on the test set\n",
    "y_prob_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "y_prob_dt = dt_model.predict_proba(X_test)[:, 1]\n",
    "y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Classify properties as successful or unsuccessful based on the optimal decision threshold\n",
    "y_pred_rf = (y_prob_rf > min_threshold).astype(int)\n",
    "y_pred_dt = (y_prob_dt > min_threshold).astype(int)\n",
    "y_pred_xgb = (y_prob_xgb > min_threshold).astype(int)\n",
    "\n",
    "# Create confusion matrices\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a list of models and their confusion matrices\n",
    "models = ['Random Forest', 'Decision Tree', 'XGBoost']\n",
    "cms = [cm_rf, cm_dt, cm_xgb]\n",
    "\n",
    "# Plot the confusion matrix for each model\n",
    "for model, cm in zip(models, cms):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {model}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Calculate metrics for Random Forest\n",
    "tn_rf, fp_rf, fn_rf, tp_rf = cm_rf.ravel()\n",
    "specificity_rf = tn_rf / (tn_rf + fp_rf)\n",
    "\n",
    "# Calculate metrics for Decision Tree\n",
    "tn_dt, fp_dt, fn_dt, tp_dt = cm_dt.ravel()\n",
    "specificity_dt = tn_dt / (tn_dt + fp_dt)\n",
    "\n",
    "# Calculate metrics for XGBoost\n",
    "tn_xgb, fp_xgb, fn_xgb, tp_xgb = cm_xgb.ravel()\n",
    "specificity_xgb = tn_xgb / (tn_xgb + fp_xgb)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate metrics for Random Forest\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "\n",
    "# Calculate metrics for Decision Tree\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "precision_dt = precision_score(y_test, y_pred_dt)\n",
    "recall_dt = recall_score(y_test, y_pred_dt)\n",
    "\n",
    "# Calculate metrics for XGBoost\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "\n",
    "\n",
    "\n",
    "# Print the metrics for each model\n",
    "print('\\nMetrics for Random Forest:')\n",
    "print(f'Accuracy: {accuracy_rf:.4f}')\n",
    "print(f'Precision: {precision_rf:.4f}')\n",
    "print(f'Recall (Sensitivity): {recall_rf:.4f}')\n",
    "print(f'Specificity: {specificity_rf:.4f}')\n",
    "\n",
    "print('\\nMetrics for Decision Tree:')\n",
    "print(f'Accuracy: {accuracy_dt:.4f}')\n",
    "print(f'Precision: {precision_dt:.4f}')\n",
    "print(f'Recall (Sensitivity): {recall_dt:.4f}')\n",
    "print(f'Specificity: {specificity_dt:.4f}')\n",
    "\n",
    "print('\\nMetrics for XGBoost:')\n",
    "print(f'Accuracy: {accuracy_xgb:.4f}')\n",
    "print(f'Precision: {precision_xgb:.4f}')\n",
    "print(f'Recall (Sensitivity): {recall_xgb:.4f}')\n",
    "print(f'Specificity: {specificity_xgb:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = rf_model\n",
    "model_dt = dt_model\n",
    "model_xgb = xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability thresholds for various models:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the costs and benefits\n",
    "benefit = 1000000\n",
    "cost = 3000000\n",
    "\n",
    "# Calculate the probabilities of the positive class for each model\n",
    "\n",
    "y_prob_rf = model_rf.predict_proba(X_test)[:, 1]\n",
    "y_prob_dt = model_dt.predict_proba(X_test)[:, 1]\n",
    "y_prob_xgb = model_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the expected cost for different decision thresholds for each model\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "expected_costs_rf = []\n",
    "for threshold in thresholds:\n",
    "    y_pred_rf = (y_prob_rf > threshold).astype(int)\n",
    "    fp_rf = np.sum((y_test == 0) & (y_pred_rf == 1))\n",
    "    tp_rf = np.sum((y_test == 1) & (y_pred_rf == 1))\n",
    "    expected_cost_rf = fp_rf * cost - tp_rf * benefit\n",
    "    expected_costs_rf.append(expected_cost_rf)\n",
    "\n",
    "# Decision Tree\n",
    "expected_costs_dt = []\n",
    "for threshold in thresholds:\n",
    "    y_pred_dt = (y_prob_dt > threshold).astype(int)\n",
    "    fp_dt = np.sum((y_test == 0) & (y_pred_dt == 1))\n",
    "    tp_dt = np.sum((y_test == 1) & (y_pred_dt == 1))\n",
    "    expected_cost_dt = fp_dt * cost - tp_dt * benefit\n",
    "    expected_costs_dt.append(expected_cost_dt)\n",
    "\n",
    "# XGBoost\n",
    "expected_costs_xgb = []\n",
    "for threshold in thresholds:\n",
    "    y_pred_xgb = (y_prob_xgb > threshold).astype(int)\n",
    "    fp_xgb = np.sum((y_test == 0) & (y_pred_xgb == 1))\n",
    "    tp_xgb = np.sum((y_test == 1) & (y_pred_xgb == 1))\n",
    "    expected_cost_xgb = fp_xgb * cost - tp_xgb * benefit\n",
    "    expected_costs_xgb.append(expected_cost_xgb)\n",
    "\n",
    "# Find the threshold with the minimum expected cost for each model\n",
    "\n",
    "\n",
    "min_cost_rf = min(expected_costs_rf)\n",
    "min_threshold_rf = thresholds[expected_costs_rf.index(min_cost_rf)]\n",
    "\n",
    "min_cost_dt = min(expected_costs_dt)\n",
    "min_threshold_dt = thresholds[expected_costs_dt.index(min_cost_dt)]\n",
    "\n",
    "min_cost_xgb = min(expected_costs_xgb)\n",
    "min_threshold_xgb = thresholds[expected_costs_xgb.index(min_cost_xgb)]\n",
    "\n",
    "# Print the minimum expected costs and optimal decision thresholds for each model\n",
    "\n",
    "print('Random Forest:')\n",
    "print(f'Minimum Expected Cost: {min_cost_rf}')\n",
    "print(f'Optimal Decision Threshold: {min_threshold_rf}\\n')\n",
    "\n",
    "print('Decision Tree:')\n",
    "print(f'Minimum Expected Cost: {min_cost_dt}')\n",
    "print(f'Optimal Decision Threshold: {min_threshold_dt}\\n')\n",
    "\n",
    "print('XGBoost:')\n",
    "print(f'Minimum Expected Cost: {min_cost_xgb}')\n",
    "print(f'Optimal Decision Threshold: {min_threshold_xgb}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary with the model metrics\n",
    "model_metrics = {\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'Decision Tree', 'XGBoost'],\n",
    "    'Accuracy': [accuracy, accuracy_rf, accuracy_dt, accuracy_xgb],\n",
    "    'Optimal Threshold': [min_threshold, min_threshold_rf, min_threshold_dt, min_threshold_xgb]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "metrics_table = pd.DataFrame(model_metrics)\n",
    "\n",
    "# Print the table\n",
    "print(metrics_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Based on the EDA and the modeling contingent on \n",
    "the cost benigit contriants provided in the Question\n",
    "\n",
    "Here are the best models:\n",
    "    \n",
    "Logistic Regression: 77% accuracy, threshold is 75%\n",
    "\n",
    "XGboost: 72% accuracy , threshold is 77%\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
